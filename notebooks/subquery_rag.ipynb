{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\dev\\simple-rag\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "\n",
    "# model_url = \"https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # model_url=\"https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf\",\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=\"./../llm/stablelm-zephyr-3b.Q4_K_M.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    embed_model=embed_model, \n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file titled: Grain size strengthening in terms of dislocation density measured by resistivity\n",
      "Storing and Indexing file title: Grain size strengthening in terms of dislocation density measured by resistivity\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n",
      "Init query engine and add to qe_tools\n",
      "Loading file titled: [Armstrong] 60 Years of Hall-Petch - Past to Present Nano-Scale Connections\n",
      "Storing and Indexing file title: [Armstrong] 60 Years of Hall-Petch - Past to Present Nano-Scale Connections\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n",
      "Init query engine and add to qe_tools\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import glob\n",
    "import chromadb\n",
    "\n",
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "def parse_string(input_string):\n",
    "    parsed_string = input_string.replace(' ', '_')\n",
    "    parsed_string = parsed_string.replace('-', '_')\n",
    "    parsed_string = re.sub(r'[^a-z0-9_]', '', parsed_string.lower())\n",
    "    parsed_string = re.sub(r'(_)\\1+', r'\\1', parsed_string)\n",
    "    \n",
    "    return parsed_string\n",
    "\n",
    "input_dir = \"./../ainu_papers/\"\n",
    "supported_files = [\".pdf\", \".csv\", \".docx\", \".txt\", \".epub\", \".hwp\", \".mbox\", \".ppt\", \".pptm\", \".pptx\", \".ipynb\", \".md\"]\n",
    "files = [f for f in glob.glob(f\"{input_dir}**/*\", recursive=True) if f\".{f.split('.')[-1]}\" in supported_files]\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./db\")\n",
    "query_engine_tools = []\n",
    "\n",
    "for file in files:\n",
    "    new = False\n",
    "    title = \" \".join(file.replace(\"\\\\\", \"/\").split(\"/\")[-1].split(\".\")[:-1])\n",
    "    collection_name = parse_string(title)[:63]\n",
    "    print(f\"Loading file titled: {title}\")\n",
    "    try:\n",
    "        chroma_collection = db.get_collection(collection_name)\n",
    "    except:\n",
    "        new = True\n",
    "        documents = SimpleDirectoryReader(input_files=[file]).load_data()\n",
    "        chroma_collection = db.create_collection(collection_name)\n",
    "\n",
    "    print(f\"Storing and Indexing file title: {title}\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "    if new:\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents, \n",
    "            storage_context=storage_context, \n",
    "            service_context=service_context,\n",
    "            use_async=True,\n",
    "        )\n",
    "    else:\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        service_context=service_context,\n",
    "        use_async=True,\n",
    "    )\n",
    "        \n",
    "    print(f\"Init query engine and add to qe_tools\")\n",
    "    # Query Data from the persisted index\n",
    "    query_engine = index.as_query_engine()\n",
    "    query_engine_tools.append(\n",
    "        QueryEngineTool(\n",
    "            query_engine=query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=collection_name,\n",
    "                description=f\"A Paper titled: {title}\",\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "subq_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    service_context=service_context,\n",
    "    use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  15.542579 seconds\n",
      "      |_CBEventType.TEMPLATING ->  0.0 seconds\n",
      "      |_CBEventType.LLM ->  15.542579 seconds\n",
      "**********\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SubQuestion\ntool_name\n  none is not an allowed value (type=type_error.none.not_allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msubq_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhy Nickel is chosen as material for the experimentation?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mf:\\dev\\simple-rag\\.env\\Lib\\site-packages\\llama_index\\core\\base_query_engine.py:30\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     29\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\dev\\simple-rag\\.env\\Lib\\site-packages\\llama_index\\query_engine\\sub_question_query_engine.py:132\u001b[0m, in \u001b[0;36mSubQuestionQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    130\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    131\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 132\u001b[0m         sub_questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_question_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m         colors \u001b[38;5;241m=\u001b[39m get_color_mapping([\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sub_questions))])\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[1;32mf:\\dev\\simple-rag\\.env\\Lib\\site-packages\\llama_index\\question_gen\\llm_generators.py:73\u001b[0m, in \u001b[0;36mLLMQuestionGenerator.generate\u001b[1;34m(self, tools, query)\u001b[0m\n\u001b[0;32m     66\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm_predictor\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     67\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[0;32m     68\u001b[0m     tools_str\u001b[38;5;241m=\u001b[39mtools_str,\n\u001b[0;32m     69\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[0;32m     70\u001b[0m )\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m parse \u001b[38;5;241m=\u001b[39m cast(StructuredOutput, parse)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse\u001b[38;5;241m.\u001b[39mparsed_output\n",
      "File \u001b[1;32mf:\\dev\\simple-rag\\.env\\Lib\\site-packages\\llama_index\\question_gen\\output_parser.py:15\u001b[0m, in \u001b[0;36mSubQuestionOutputParser.parse\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_dict:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid JSON found in output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m sub_questions \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mSubQuestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson_dict\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredOutput(raw_output\u001b[38;5;241m=\u001b[39moutput, parsed_output\u001b[38;5;241m=\u001b[39msub_questions)\n",
      "File \u001b[1;32mf:\\dev\\simple-rag\\.env\\Lib\\site-packages\\llama_index\\question_gen\\output_parser.py:15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_dict:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid JSON found in output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m sub_questions \u001b[38;5;241m=\u001b[39m [\u001b[43mSubQuestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m json_dict]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredOutput(raw_output\u001b[38;5;241m=\u001b[39moutput, parsed_output\u001b[38;5;241m=\u001b[39msub_questions)\n",
      "File \u001b[1;32mf:\\dev\\simple-rag\\.env\\Lib\\site-packages\\pydantic\\v1\\main.py:526\u001b[0m, in \u001b[0;36mBaseModel.parse_obj\u001b[1;34m(cls, obj)\u001b[0m\n\u001b[0;32m    524\u001b[0m         exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected dict not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ValidationError([ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\dev\\simple-rag\\.env\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for SubQuestion\ntool_name\n  none is not an allowed value (type=type_error.none.not_allowed)"
     ]
    }
   ],
   "source": [
    "response = subq_engine.query(\"Why Nickel is chosen as material for the experimentation?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub Question 0: What are the advantages of using nickel in experimental research?\n",
      "Answer: Nickel has several advantages when used in experimental research, including:\n",
      "1. High stacking fault energy: Nickel has a high stacking fault energy, which makes it suitable for studying dislocation density and its effect on flow stress.\n",
      "2. Easy to work with: Nickel is a relatively easy material to work with, as it can be easily melted and shaped into various forms.\n",
      "3. High electrical resistivity: Nickel has a high electrical resistivity, which allows for the measurement of dislocation density using electrical resistivity.\n",
      "4. Low thermal conductivity: Nickel has a low thermal conductivity, which makes it suitable for studying the effect of temperature on dislocation density.\n",
      "5. High ductility: Nickel is highly ductile, which allows for the study of plastic deformation and its effect on dislocation density.\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# iterate through sub_question items captured in SUB_QUESTION event\n",
    "from llama_index.callbacks.schema import CBEventType, EventPayload\n",
    "\n",
    "for i, (start_event, end_event) in enumerate(\n",
    "    llama_debug.get_event_pairs(CBEventType.SUB_QUESTION)\n",
    "):\n",
    "    qa_pair = end_event.payload[EventPayload.SUB_QUESTION]\n",
    "    print(\"Sub Question \" + str(i) + \": \" + qa_pair.sub_q.sub_question.strip())\n",
    "    print(\"Answer: \" + qa_pair.answer.strip())\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
